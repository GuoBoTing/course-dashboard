#!/usr/bin/env python3
"""
èª²ç¨‹è¶¨å‹¢å„€è¡¨æ¿ (Streamlit)
é¡¯ç¤º Hahow èˆ‡ PressPlay çš„èª²ç¨‹æ’åã€æˆé•·ç‡ã€æˆé•·é€Ÿåº¦ï¼Œä¸¦æ¨™ç¤ºè¿‘æœŸæˆé•·å¿«é€Ÿçš„èª²ç¨‹ã€‚
"""
import subprocess
import sys
import os
from datetime import date
from pathlib import Path
from dotenv import load_dotenv
import pandas as pd
import streamlit as st
import plotly.express as px

load_dotenv(Path(__file__).parent / ".env")

def get_secret(key: str) -> str:
    """å„ªå…ˆè®€ Streamlit Secretsï¼ˆCloudï¼‰ï¼Œfallback åˆ° .envï¼ˆæœ¬åœ°ï¼‰ã€‚"""
    try:
        return st.secrets[key]
    except Exception:
        return os.getenv(key, "")

@st.cache_resource
def get_supabase():
    from supabase import create_client
    url = get_secret("SUPABASE_URL")
    key = get_secret("SUPABASE_KEY")
    if not url or not key:
        return None
    return create_client(url, key)

def get_last_scrape_date() -> date | None:
    """è®€å– Supabase ä¸­æœ€æ–°ä¸€ç­†çš„çˆ¬å–æ—¥æœŸã€‚"""
    sb = get_supabase()
    if sb is None:
        return None
    try:
        res = sb.table("course_scrapes").select("scraped_at").order("scraped_at", desc=True).limit(1).execute()
        if not res.data:
            return None
        return pd.to_datetime(res.data[0]["scraped_at"]).date()
    except Exception:
        return None

PLATFORM_LABEL = {"hahow": "Hahow", "pressplay": "PressPlay"}
PLATFORM_COLOR = {"hahow": "#FF6B35", "pressplay": "#3A86FF"}

# â”€â”€ é é¢è¨­å®š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

st.set_page_config(
    page_title="èª²ç¨‹è¶¨å‹¢å„€è¡¨æ¿",
    page_icon="ğŸ“ˆ",
    layout="wide",
)

st.title("ğŸ“ˆ èª²ç¨‹è¶¨å‹¢å„€è¡¨æ¿")
st.caption("è¿½è¹¤ Hahow & PressPlay ç†±é–€èª²ç¨‹çš„æˆé•·ç‡èˆ‡æˆé•·é€Ÿåº¦")

# â”€â”€ çˆ¬å–æŒ‰éˆ•ï¼ˆæœ€å…ˆå®šç¾©ï¼Œç¢ºä¿ç„¡è³‡æ–™æ™‚ä¹Ÿèƒ½é¡¯ç¤ºï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

with st.sidebar:
    st.header("è³‡æ–™æ›´æ–°")

    project_dir = Path(__file__).parent
    scraper_path = project_dir / "scraper.py"
    venv_python = project_dir / ".venv" / "bin" / "python"
    python_exec = str(venv_python) if venv_python.exists() else sys.executable

    # â”€â”€ ç®¡ç†å“¡ç™»å…¥ â”€â”€
    if "is_admin" not in st.session_state:
        st.session_state.is_admin = False

    if not st.session_state.is_admin:
        with st.form("login_form"):
            pwd = st.text_input("ç®¡ç†å“¡å¯†ç¢¼", type="password")
            if st.form_submit_button("ç™»å…¥", width="stretch"):
                if pwd == get_secret("ADMIN_PASSWORD"):
                    st.session_state.is_admin = True
                    st.rerun()
                else:
                    st.error("å¯†ç¢¼éŒ¯èª¤")
    else:
        st.caption(f"ğŸ‘¤ ç®¡ç†å“¡å·²ç™»å…¥")
        if st.button("ç™»å‡º", width="stretch"):
            st.session_state.is_admin = False
            st.rerun()

        st.divider()

        def run_scraper(extra_args: list, spinner_msg: str):
            with st.spinner(spinner_msg):
                proc = subprocess.run(
                    [python_exec, str(scraper_path)] + extra_args,
                    capture_output=True,
                    text=True,
                    cwd=str(project_dir),
                )
            if proc.returncode == 0:
                st.success("å®Œæˆï¼")
                st.text(proc.stdout[-800:] if len(proc.stdout) > 800 else proc.stdout)
                st.cache_data.clear()
                st.rerun()
            else:
                st.error("çˆ¬å–å¤±æ•—")
                if proc.stdout:
                    st.text("--- stdout ---\n" + (proc.stdout[-1200:] if len(proc.stdout) > 1200 else proc.stdout))
                if proc.stderr:
                    st.text("--- stderr ---\n" + (proc.stderr[-800:] if len(proc.stderr) > 800 else proc.stderr))

        last_scrape = get_last_scrape_date()
        if last_scrape:
            st.caption(f"ä¸Šæ¬¡æ›´æ–°ï¼š{last_scrape}")

        run_update   = st.button("ğŸ”„ æ›´æ–°å­¸ç”Ÿæ•¸",   width="stretch", type="primary",
                                 help="åªæ›´æ–°å­¸ç”Ÿäººæ•¸ï¼Œmarkdown æ¨¡å¼ï¼ˆ~20 creditsï¼‰")
        run_discover = st.button("ğŸ” é‡æ–°ç™¼ç¾èª²ç¨‹", width="stretch",
                                 help="é‡æ–°çˆ¬åˆ—è¡¨é å–å¾—æœ€æ–°æ’åï¼ŒLLM æ¨¡å¼ï¼ˆ~60 creditsï¼‰")
        if run_update:
            run_scraper([], "æ›´æ–°å­¸ç”Ÿæ•¸ä¸­ï¼ˆç´„ 2ï½4 åˆ†é˜ï¼‰â€¦")
        if run_discover:
            run_scraper(["--discover"], "é‡æ–°ç™¼ç¾èª²ç¨‹ä¸­ï¼ˆç´„ 3ï½5 åˆ†é˜ï¼‰â€¦")

    st.divider()

# â”€â”€ è¼‰å…¥è³‡æ–™ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@st.cache_data(ttl=120)
def load_data() -> pd.DataFrame:
    sb = get_supabase()
    if sb is None:
        st.error("å°šæœªè¨­å®š Supabase é€£ç·šï¼Œè«‹åœ¨ Secrets ä¸­åŠ å…¥ SUPABASE_URL å’Œ SUPABASE_KEYã€‚")
        return pd.DataFrame()
    try:
        res = sb.table("course_scrapes").select("*").order("scraped_at").execute()
        if not res.data:
            return pd.DataFrame()
        df = pd.DataFrame(res.data)
        df["scraped_at"] = pd.to_datetime(df["scraped_at"])
        df["students"]   = pd.to_numeric(df["students"], errors="coerce")
        df["price"]      = pd.to_numeric(df["price"],    errors="coerce")
        df["rank"]       = pd.to_numeric(df["rank"],     errors="coerce")
        return df
    except Exception as e:
        st.error(f"è³‡æ–™åº«è®€å–å¤±æ•—ï¼š{e}")
        return pd.DataFrame()

df = load_data()

if df.empty:
    st.info("å°šç„¡è³‡æ–™ï¼Œè«‹é»æ“Šå·¦å´ã€ŒğŸ”„ ç«‹å³çˆ¬å–ã€æŒ‰éˆ•é–‹å§‹æ”¶é›†èª²ç¨‹è³‡æ–™ã€‚")
    st.stop()

# â”€â”€ å´é‚Šæ¬„ç¯©é¸ï¼ˆæœ‰è³‡æ–™å¾Œæ‰é¡¯ç¤ºï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

with st.sidebar:
    st.header("ç¯©é¸æ¢ä»¶")

    platform_options = ["å…¨éƒ¨"] + [PLATFORM_LABEL[p] for p in sorted(df["platform"].unique())]
    selected_label = st.selectbox("å¹³å°", platform_options)

    all_dates = sorted(df["scraped_at"].dt.date.unique())
    if len(all_dates) >= 2:
        date_range = st.date_input(
            "æ—¥æœŸç¯„åœ",
            value=(all_dates[0], all_dates[-1]),
            min_value=all_dates[0],
            max_value=all_dates[-1],
        )
        if isinstance(date_range, tuple) and len(date_range) == 2:
            start_date, end_date = date_range
        else:
            start_date = end_date = all_dates[-1]
    else:
        start_date = end_date = all_dates[0]
        st.info(f"ç›®å‰åƒ…æœ‰ {all_dates[0]} çš„è³‡æ–™")

    growth_threshold = st.slider("æˆé•·ç‡æç¤ºé–€æª»ï¼ˆ%ï¼‰", min_value=1, max_value=50, value=5)

# â”€â”€ å¥—ç”¨ç¯©é¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

df_f = df.copy()
if selected_label != "å…¨éƒ¨":
    inv_map = {v: k for k, v in PLATFORM_LABEL.items()}
    df_f = df_f[df_f["platform"] == inv_map[selected_label]]

df_f = df_f[
    (df_f["scraped_at"].dt.date >= start_date) &
    (df_f["scraped_at"].dt.date <= end_date)
]

# â”€â”€ è¨ˆç®—æˆé•·æŒ‡æ¨™ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def compute_growth(df: pd.DataFrame) -> pd.DataFrame:
    """
    ä»¥ã€Œå¤©ã€ç‚ºå–®ä½è¨ˆç®—æˆé•·æŒ‡æ¨™ï¼š
    - å…ˆå°‡æ¯é–€èª²æŒ‰æ—¥æœŸå½™ç¸½ï¼ˆæ¯å¤©å–æœ€å¾Œä¸€ç­†ï¼‰ï¼Œé¿å…åŒæ—¥å¤šæ¬¡çˆ¬å–å¹²æ“¾
    - student_diff : é¦–æ—¥ â†’ æœ«æ—¥çš„å­¸ç”Ÿäººæ•¸è®ŠåŒ–é‡
    - growth_rate  : æˆé•·ç‡ (%)
    - growth_speed : æ¯å¤©æˆé•·äººæ•¸ (äºº/å¤©)ï¼Œéœ€è‡³å°‘è·¨è¶Š 2 å€‹ä¸åŒæ—¥æœŸ
    - days_elapsed : è§€å¯Ÿå¤©æ•¸ï¼ˆæ—¥æ›†å¤©æ•¸å·®ï¼‰
    """
    rows = []
    for (platform, course_name), grp in df.groupby(["platform", "course_name"]):
        grp = grp.sort_values("scraped_at")

        # æŒ‰æ—¥å½™ç¸½ï¼šæ¯å¤©å–æœ€å¾Œä¸€ç­†ï¼ˆæœ€æ–°çš„çˆ¬å–çµæœï¼‰
        grp["_date"] = grp["scraped_at"].dt.date
        daily = (
            grp.groupby("_date", sort=True)
               .last()
               .reset_index()
        )

        first = daily.iloc[0]
        last  = daily.iloc[-1]

        s0 = first["students"]
        s1 = last["students"]

        # æ—¥æ›†å¤©æ•¸å·®ï¼ˆä¸æ˜¯å°æ™‚å·®ï¼‰
        day_diff = (last["_date"] - first["_date"]).days

        diff  = (s1 - s0)           if pd.notna(s0) and pd.notna(s1) else None
        rate  = (diff / s0 * 100)   if diff is not None and s0 and s0 > 0 else None
        # æˆé•·é€Ÿåº¦åªåœ¨æœ‰è·¨æ—¥è³‡æ–™æ™‚æ‰è¨ˆç®—
        speed = (diff / day_diff)   if diff is not None and day_diff >= 1 else None

        rows.append({
            "platform":        platform,
            "course_name":     course_name,
            "teacher":         last["teacher"],
            "latest_students": s1,
            "latest_price":    last["price"],
            "latest_rank":     last["rank"] if "rank" in grp.columns else None,
            "student_diff":    diff,
            "growth_rate":     rate,
            "growth_speed":    speed,
            "days_elapsed":    day_diff if day_diff >= 1 else None,
            "scrape_count":    len(daily),
            "course_url":      last.get("course_url", "") or "",
        })
    return pd.DataFrame(rows)

growth_df = compute_growth(df_f)

# â”€â”€ ç¸½è¦½æŒ‡æ¨™ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

latest_time    = df["scraped_at"].max()
unique_courses = df[["platform", "course_name"]].drop_duplicates().shape[0]
scrape_count   = df["scraped_at"].nunique()

c1, c2, c3 = st.columns(3)
c1.metric("è¿½è¹¤èª²ç¨‹ç¸½æ•¸", unique_courses)
c2.metric("ç´¯è¨ˆçˆ¬å–æ¬¡æ•¸", scrape_count)
c3.metric("æœ€å¾Œæ›´æ–°æ™‚é–“", latest_time.strftime("%Y-%m-%d %H:%M"))

st.divider()

# â”€â”€ æˆé•·å¿«é€Ÿæç¤º â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

st.subheader("ğŸš€ è¿‘æœŸæˆé•·å¿«é€Ÿèª²ç¨‹")

fast = (
    growth_df[
        growth_df["growth_rate"].notna() &
        (growth_df["growth_rate"] >= growth_threshold)
    ]
    .sort_values("growth_speed", ascending=False)
    .head(5)
)

if fast.empty:
    unique_days = df_f["scraped_at"].dt.date.nunique()
    if unique_days < 2:
        st.info("éœ€è¦è‡³å°‘ **å…©å¤©** çš„è³‡æ–™æ‰èƒ½è¨ˆç®—æˆé•·ç‡ã€‚")
    else:
        st.info(f"ç›®å‰ç„¡èª²ç¨‹æˆé•·ç‡è¶…é {growth_threshold}%ï¼Œå¯èª¿æ•´å´é‚Šæ¬„çš„é–€æª»å€¼ã€‚")
else:
    for _, row in fast.iterrows():
        plabel = PLATFORM_LABEL.get(row["platform"], row["platform"])
        rate_str  = f"+{row['growth_rate']:.1f}%"
        speed_str = f"+{row['growth_speed']:,.1f} äºº/å¤©" if pd.notna(row["growth_speed"]) else "â€”"
        diff_str  = f"+{int(row['student_diff']):,} äºº"

        with st.container(border=True):
            col1, col2, col3, col4 = st.columns([4, 1, 1, 1])
            url = row.get("course_url", "") or ""
            name_md = f"[{row['course_name']}]({url})" if url else row['course_name']
            col1.markdown(
                f"**{name_md}**  \n"
                f"<span style='color:gray'>{row['teacher']} ï½œ {plabel}</span>",
                unsafe_allow_html=True,
            )
            col2.metric("ç›®å‰å­¸ç”Ÿ", f"{int(row['latest_students']):,}" if pd.notna(row['latest_students']) else "N/A")
            col3.metric("æˆé•·ç‡",   rate_str,  delta=diff_str)
            col4.metric("æˆé•·é€Ÿåº¦", speed_str)

st.divider()

# â”€â”€ æœ€æ–°æ’åè¡¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

st.subheader("ğŸ“‹ æœ€æ–°èª²ç¨‹æ’å")

latest_snap = df_f[df_f["scraped_at"] == df_f["scraped_at"].max()].copy()

if latest_snap.empty:
    st.warning("ç¯©é¸å¾Œç„¡æœ€æ–°è³‡æ–™ã€‚")
else:
    table = latest_snap.merge(
        growth_df[["platform", "course_name", "growth_rate", "growth_speed", "student_diff"]],
        on=["platform", "course_name"],
        how="left",
    )
    table["å¹³å°"]       = table["platform"].map(PLATFORM_LABEL)
    table["æˆé•·ç‡(%)"]  = table["growth_rate"].apply(
        lambda x: f"+{x:.1f}" if pd.notna(x) and x > 0 else (f"{x:.1f}" if pd.notna(x) else "â€”")
    )
    table["æˆé•·é€Ÿåº¦(äºº/å¤©)"] = table["growth_speed"].apply(
        lambda x: f"+{x:,.1f}" if pd.notna(x) and x > 0 else (f"{x:,.1f}" if pd.notna(x) else "â€”")
    )

    display = (
        table
        .sort_values("growth_rate", ascending=False, na_position="last")
        [[
            "å¹³å°", "rank", "course_name", "teacher",
            "price", "students", "æˆé•·ç‡(%)", "æˆé•·é€Ÿåº¦(äºº/å¤©)", "course_url"
        ]]
        .rename(columns={
            "rank":        "æ’å",
            "course_name": "èª²ç¨‹åç¨±",
            "teacher":     "è€å¸«",
            "price":       "åƒ¹æ ¼(NTD)",
            "students":    "å­¸ç”Ÿæ•¸",
            "course_url":  "é€£çµ",
        })
    )

    st.dataframe(
        display,
        column_config={
            "é€£çµ": st.column_config.LinkColumn("é€£çµ", display_text="ğŸ”— é–‹å•Ÿ"),
        },
        width="stretch",
        hide_index=True,
    )

st.divider()

# â”€â”€ è¶¨å‹¢æŠ˜ç·šåœ– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

st.subheader("ğŸ“‰ å­¸ç”Ÿäººæ•¸è¶¨å‹¢ï¼ˆæ¯æ—¥ï¼‰")

unique_days = df_f["scraped_at"].dt.date.nunique()
if unique_days < 2:
    st.info("éœ€è¦è‡³å°‘å…©å¤©çš„è³‡æ–™æ‰èƒ½é¡¯ç¤ºè¶¨å‹¢åœ–ã€‚")
else:
    top_default = (
        growth_df[growth_df["latest_students"].notna()]
        .nlargest(5, "latest_students")["course_name"]
        .tolist()
    )
    all_course_names = sorted(growth_df["course_name"].unique().tolist())

    selected = st.multiselect(
        "é¸æ“‡è¦æ¯”è¼ƒçš„èª²ç¨‹ï¼ˆæœ€å¤š 10 é–€ï¼‰",
        options=all_course_names,
        default=[c for c in top_default if c in all_course_names],
        max_selections=10,
    )

    if selected:
        trend_raw = df_f[df_f["course_name"].isin(selected)].copy()
        trend_raw["date"] = trend_raw["scraped_at"].dt.date

        # æ¯å¤©å–æœ€å¾Œä¸€ç­†ï¼Œä»¥ã€Œå¤©ã€ç‚ºç²’åº¦é¡¯ç¤º
        trend = (
            trend_raw.sort_values("scraped_at")
                     .groupby(["platform", "course_name", "date"], sort=True)
                     .last()
                     .reset_index()
        )
        trend["èª²ç¨‹"] = trend["platform"].map(PLATFORM_LABEL) + " Â· " + trend["course_name"]

        fig = px.line(
            trend,
            x="date",
            y="students",
            color="èª²ç¨‹",
            markers=True,
            title="å­¸ç”Ÿäººæ•¸æ­·å²è¶¨å‹¢ï¼ˆæ¯æ—¥ï¼‰",
            labels={"date": "æ—¥æœŸ", "students": "å­¸ç”Ÿäººæ•¸"},
        )
        fig.update_layout(hovermode="x unified", legend_title="èª²ç¨‹")
        st.plotly_chart(fig, width="stretch")

    # â”€â”€ æˆé•·é€Ÿåº¦é•·æ¢åœ– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    st.subheader("âš¡ æˆé•·é€Ÿåº¦æ¯”è¼ƒï¼ˆæ¯æ—¥æ–°å¢å­¸ç”Ÿï¼‰")

    top_speed = (
        growth_df[growth_df["growth_speed"].notna() & (growth_df["growth_speed"] > 0)]
        .nlargest(15, "growth_speed")
        .copy()
    )

    if top_speed.empty:
        st.info("å°šç„¡è·¨æ—¥æ­£å‘æˆé•·è³‡æ–™å¯é¡¯ç¤ºã€‚")
    else:
        top_speed["label"] = (
            top_speed["platform"].map(PLATFORM_LABEL) + " Â· " +
            top_speed["course_name"].str[:25]
        )
        fig2 = px.bar(
            top_speed,
            x="growth_speed",
            y="label",
            orientation="h",
            color="platform",
            color_discrete_map=PLATFORM_COLOR,
            title="æ¯æ—¥å­¸ç”Ÿæˆé•·é€Ÿåº¦ Top 15",
            labels={
                "growth_speed": "æ¯æ—¥æˆé•·äººæ•¸",
                "label": "",
                "platform": "å¹³å°",
            },
            text="growth_speed",
        )
        fig2.update_traces(texttemplate="%{text:,.1f}", textposition="outside")
        fig2.update_layout(
            yaxis={"categoryorder": "total ascending"},
            xaxis_title="æ¯æ—¥æ–°å¢å­¸ç”Ÿäººæ•¸",
            showlegend=True,
        )
        st.plotly_chart(fig2, width="stretch")

# â”€â”€ è³‡æ–™ç®¡ç†ï¼ˆç®¡ç†å“¡é™å®šï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if st.session_state.get("is_admin"):
    st.divider()
    st.subheader("ğŸ—‚ è³‡æ–™ç®¡ç†")

    mgmt_platform = st.selectbox(
        "å¹³å°",
        options=sorted(df["platform"].unique()),
        format_func=lambda x: PLATFORM_LABEL.get(x, x),
        key="mgmt_platform",
    )
    courses_in_platform = sorted(
        df[df["platform"] == mgmt_platform]["course_name"].unique()
    )
    mgmt_course = st.selectbox("èª²ç¨‹", options=courses_in_platform, key="mgmt_course")

    course_hist = (
        df[(df["platform"] == mgmt_platform) & (df["course_name"] == mgmt_course)]
        [["id", "scraped_at", "students", "rank", "price", "course_url"]]
        .sort_values("scraped_at")
        .reset_index(drop=True)
    )

    # é¡¯ç¤ºæ™‚ä¸é¡¯ç¤º id æ¬„ï¼Œä½†ä¿ç•™åœ¨ DataFrame ä¾›åˆªé™¤ä½¿ç”¨
    st.dataframe(
        course_hist.drop(columns=["id"]),
        width="stretch",
        hide_index=False,
    )

    ts_options = course_hist["scraped_at"].dt.strftime("%Y-%m-%d %H:%M:%S").tolist()
    to_delete = st.multiselect("é¸æ“‡è¦åˆªé™¤çš„ç´€éŒ„ï¼ˆå¯å¤šé¸ï¼‰", options=ts_options, key="mgmt_delete")

    if to_delete:
        st.warning(f"å°‡åˆªé™¤ {len(to_delete)} ç­†ç´€éŒ„ï¼Œæ­¤æ“ä½œç„¡æ³•é‚„åŸã€‚")
        if st.button("ğŸ—‘ ç¢ºèªåˆªé™¤", type="primary"):
            delete_set = set(to_delete)
            ids_to_delete = course_hist[
                course_hist["scraped_at"].dt.strftime("%Y-%m-%d %H:%M:%S").isin(delete_set)
            ]["id"].tolist()
            sb = get_supabase()
            sb.table("course_scrapes").delete().in_("id", ids_to_delete).execute()
            st.success(f"å·²åˆªé™¤ {len(ids_to_delete)} ç­†ç´€éŒ„")
            st.cache_data.clear()
            st.rerun()

    st.divider()
    st.markdown("**ğŸ§¹ æ‰¹æ¬¡æ¸…é™¤**")

    svc_ids = df[df["course_url"].str.contains("/services/", na=False)]["id"].tolist()
    null_ids = df[df["students"].isna()]["id"].tolist()

    col_a, col_b = st.columns(2)
    with col_a:
        st.caption(f"æœå‹™/å·¥ä½œåŠé é¢ï¼š{len(svc_ids)} ç­†")
        if st.button("ğŸ—‘ æ¸…é™¤æœå‹™/å·¥ä½œåŠè³‡æ–™", disabled=len(svc_ids) == 0):
            get_supabase().table("course_scrapes").delete().in_("id", svc_ids).execute()
            st.success(f"å·²æ¸…é™¤ {len(svc_ids)} ç­†")
            st.cache_data.clear()
            st.rerun()
    with col_b:
        st.caption(f"å­¸ç”Ÿæ•¸ç‚ºç©ºï¼š{len(null_ids)} ç­†")
        if st.button("ğŸ—‘ æ¸…é™¤å­¸ç”Ÿæ•¸ç©ºå€¼è³‡æ–™", disabled=len(null_ids) == 0):
            get_supabase().table("course_scrapes").delete().in_("id", null_ids).execute()
            st.success(f"å·²æ¸…é™¤ {len(null_ids)} ç­†")
            st.cache_data.clear()
            st.rerun()

# â”€â”€ é å°¾ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

st.divider()
st.caption(
    "è³‡æ–™ä¾†æºï¼šHahow & PressPlay ï½œ "
    "æ¯æ—¥åŸ·è¡Œ `python scraper.py` æ›´æ–° ï½œ "
    "å•Ÿå‹•çœ‹æ¿ï¼š`streamlit run app.py`"
)
